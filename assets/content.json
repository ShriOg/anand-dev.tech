{
  "site": {
    "name": "Anand Dev",
    "tagline": "Developer building systems that think.",
    "logo": {
      "text": "anand",
      "highlight": ".",
      "suffix": "dev"
    }
  },

  "meta": {
    "title": "Anand Dev OS | Developer",
    "description": "Anand Shukla - Developer building thoughtful systems. Portfolio, Lab, and Learning Log.",
    "themeColor": "#0a0a0b",
    "ogImage": "/assets/images/og-image.png"
  },

  "author": {
    "name": "Anand Shukla",
    "title": "Developer",
    "tagline": "Developer building systems that think.",
    "description": "Focused on Python, computer vision, and making complex things feel simple.",
    "philosophy": "Build to understand. Ship to learn. Document everything.",
    "email": "hello@anand.dev",
    "github": "https://github.com/ShriOg",
    "linkedin": "https://linkedin.com/in/anandshukla",
    "availability": "Available for projects"
  },

  "navigation": [
    { "id": "home", "label": "Home", "href": "/", "status": "active" },
    { "id": "projects", "label": "Projects", "href": "/pages/projects/", "status": "active" },
    { "id": "lab", "label": "Lab", "href": "/pages/lab/", "status": "active" },
    { "id": "dev-os", "label": "Dev OS", "href": "/pages/dev-os/", "status": "active" },
    { "id": "hire", "label": "Hire Me", "href": "/pages/hire/", "status": "active" }
  ],

  "hero": {
    "eyebrow": "Available for projects",
    "title": "Anand Shukla",
    "subtitle": "Developer building systems that think. Focused on Python, computer vision, and making complex things feel simple.",
    "philosophy": "Build to understand. Ship to learn. Document everything.",
    "cta": {
      "primary": { "label": "View Projects", "href": "/pages/projects/" },
      "secondary": { "label": "Work With Me", "href": "/pages/hire/" }
    }
  },

  "sections": {
    "featuredProjects": {
      "eyebrow": "Featured Work",
      "title": "Selected Projects"
    },
    "projects": {
      "eyebrow": "Projects",
      "title": "Engineering Stories",
      "description": "Not just what I built, but how I thought through problems, what failed, and what I learned along the way. Click any card to explore the full case study."
    },
    "skills": {
      "categories": [
        {
          "title": "Languages",
          "items": ["Python", "JavaScript", "TypeScript", "HTML/CSS", "SQL"]
        },
        {
          "title": "Domains",
          "items": ["Computer Vision", "Machine Learning", "Systems Design", "Web Development"]
        },
        {
          "title": "Tools",
          "items": ["Git", "VS Code", "Docker", "Linux", "MediaPipe", "OpenCV"]
        }
      ]
    }
  },

  "projects": [
    {
      "id": "ai-desktop-assistant",
      "title": "AI Desktop Assistant",
      "shortDescription": "A voice-controlled assistant that understands context and executes system commands across Windows, macOS, and Linux.",
      "longDescription": "Building a voice-controlled assistant that actually understands context and executes system commands across Windows, macOS, and Linux.",
      "previewType": "grid",
      "techStack": ["Python 3.11", "Pydantic", "SpeechRecognition", "pywin32", "subprocess"],
      "techStackShort": "Python / NLP",
      "timeline": "3 months",
      "role": "Solo Developer",
      "liveUrl": null,
      "sourceUrl": "https://github.com/ShriOg/anand-dev.tech/tree/main/projects/ai-assistant",
      "pageUrl": "/pages/ai-assistant/",
      "status": "active",
      "featured": true,
      "order": 1,
      "sections": [
        {
          "id": "problem",
          "title": "Problem Statement",
          "content": [
            "Existing voice assistants are either cloud-dependent, platform-locked, or require complex setup. I wanted something that could run locally, understand natural variations of commands, and actually control my computer.",
            "The core challenge: How do you map fuzzy human language to precise system commands while maintaining context across a conversation?"
          ]
        },
        {
          "id": "constraints",
          "title": "Constraints",
          "type": "list",
          "content": [
            "Must run entirely offline after initial setup",
            "Cross-platform support (Windows, macOS, Linux)",
            "Response time under 500ms for common commands",
            "Extensible action system without code changes",
            "No external API dependencies for core functionality"
          ]
        },
        {
          "id": "architecture",
          "title": "Architecture",
          "type": "diagram",
          "diagramTitle": "System Overview",
          "diagram": "Voice Input --> Speech-to-Text --> Intent Parser --> Router\n                                                       |\n                                        +--------------+--------------+\n                                        |              |              |\n                                    Context       Action Schema    Adapter\n                                    Manager       Validator       (OS-specific)\n                                        |              |              |\n                                        +--------------+--------------+\n                                                       |\n                                                   Executor --> System",
          "content": [
            "The key insight was separating intent parsing from action execution. The router maintains conversation context while the adapters handle platform-specific implementations."
          ]
        },
        {
          "id": "tradeoffs",
          "title": "Tradeoffs",
          "type": "table",
          "headers": ["Decision", "Benefit", "Cost"],
          "rows": [
            ["Local-first processing", "Privacy, offline capability", "Limited NLP accuracy vs cloud"],
            ["Schema-based actions", "Type safety, validation", "More boilerplate per action"],
            ["Adapter pattern for OS", "Clean abstraction", "Maintenance across 3 platforms"]
          ]
        },
        {
          "id": "failures",
          "title": "What Failed",
          "content": [
            "<strong>First attempt at intent parsing:</strong> Started with regex patterns. Worked for exact matches but completely broke with any variation. \"Open Chrome\" worked, \"launch chrome browser\" didn't.",
            "<strong>Context management v1:</strong> Stored everything in a flat dictionary. Memory grew unbounded and older context polluted new interactions.",
            "<strong>Windows adapter initial approach:</strong> Used subprocess for everything. Some commands need COM objects, others need PowerShell. Unified interface was naive."
          ]
        },
        {
          "id": "outcome",
          "title": "Final Outcome",
          "type": "list",
          "content": [
            "Working assistant with 50+ supported commands",
            "Average response time: 180ms",
            "Intent recognition accuracy: ~85% on varied phrasing",
            "Clean plugin system for adding new actions",
            "Documented API for external integrations"
          ]
        },
        {
          "id": "improvements",
          "title": "What I Would Improve",
          "type": "list",
          "content": [
            "Add lightweight local LLM option for better intent parsing",
            "Implement action chaining (e.g., \"open browser and go to github\")",
            "Better error recovery with suggested alternatives",
            "Visual feedback system for non-terminal usage"
          ]
        }
      ]
    },
    {
      "id": "particle-system-gestures",
      "title": "Gesture-Controlled Particles",
      "shortDescription": "An interactive visual system where hand gestures control particle behavior in real-time using computer vision.",
      "longDescription": "An interactive visual system where hand gestures control particle behavior in real-time. Exploring the intersection of computer vision and creative coding.",
      "previewType": "particles",
      "techStack": ["Vanilla JavaScript", "Canvas 2D", "MediaPipe Hands", "requestAnimationFrame"],
      "techStackShort": "JS / Canvas / MediaPipe",
      "timeline": "6 weeks",
      "role": "Solo Developer",
      "liveUrl": "/projects/particle-system-gestures/index.html",
      "sourceUrl": "https://github.com/ShriOg/particle-system-guestures",
      "pageUrl": "/pages/particle-system/",
      "status": "active",
      "featured": true,
      "order": 2,
      "sections": [
        {
          "id": "problem",
          "title": "Problem Statement",
          "content": [
            "Most interactive demos use mouse/touch input. I wanted to explore how gesture recognition could create more intuitive and expressive interactions.",
            "The challenge: MediaPipe runs at ~30fps for hand tracking, but smooth particle animation needs 60fps. How do you bridge this gap without visible stuttering?"
          ]
        },
        {
          "id": "constraints",
          "title": "Constraints",
          "type": "list",
          "content": [
            "Must run in browser without plugins",
            "60fps rendering minimum",
            "Support 500+ simultaneous particles",
            "Gesture detection latency under 100ms perceived",
            "Graceful degradation on lower-end devices"
          ]
        },
        {
          "id": "architecture",
          "title": "Architecture",
          "type": "diagram",
          "diagramTitle": "Pipeline",
          "diagram": "Camera --> MediaPipe --> Gesture      --> Particle System\n           (30fps)     Interpreter        (60fps)\n                           |                  |\n                      Hand State         Physics Engine\n                      Interpolator       Spatial Hash\n                           |                  |\n                       Smoothed          Optimized\n                       Position          Collision",
          "content": [
            "Key innovation: Position interpolation between MediaPipe frames. The gesture interpreter predicts hand position for intermediate frames, creating smooth 60fps interaction from 30fps tracking."
          ]
        },
        {
          "id": "tradeoffs",
          "title": "Tradeoffs",
          "type": "table",
          "headers": ["Decision", "Benefit", "Cost"],
          "rows": [
            ["Canvas 2D over WebGL", "Simpler code, wider support", "Particle limit ~800 vs thousands"],
            ["Position interpolation", "Smooth 60fps feel", "Slight prediction errors visible"],
            ["Spatial hashing", "O(n) collision detection", "Memory overhead for grid"]
          ]
        },
        {
          "id": "failures",
          "title": "What Failed",
          "content": [
            "<strong>Direct MediaPipe coordinates:</strong> Hand jitter made particles shake uncontrollably. Needed velocity-based smoothing with configurable damping.",
            "<strong>Naive collision detection:</strong> O(n²) killed performance at 300+ particles. Spatial hashing brought it back to 60fps.",
            "<strong>Gesture recognition complexity:</strong> Started with 10 gestures. Too many false positives. Reduced to 4 reliable gestures with clear visual states."
          ]
        },
        {
          "id": "outcome",
          "title": "Final Outcome",
          "type": "list",
          "content": [
            "Smooth 60fps with 500 particles",
            "4 gesture types: attract, repel, swirl, explode",
            "Perceived latency under 50ms",
            "Works on mobile with touch fallback",
            "Configurable physics parameters via UI"
          ]
        },
        {
          "id": "improvements",
          "title": "What I Would Improve",
          "type": "list",
          "content": [
            "WebGL renderer for 10x particle count",
            "WebWorker for physics calculations",
            "Custom gesture training for user-defined interactions",
            "Audio-reactive particle behaviors"
          ]
        }
      ]
    },
    {
      "id": "nowhang",
      "title": "NowHang",
      "shortDescription": "A social coordination app that eliminates planning friction by matching real-time availability and interests.",
      "longDescription": "A social coordination app that makes spontaneous meetups actually happen. Eliminates the back-and-forth of planning by matching real-time availability with shared interests.",
      "previewType": "waves",
      "techStack": ["React Native", "Firebase", "Cloud Functions", "Real-time DB"],
      "techStackShort": "Full Stack / Real-time",
      "timeline": "Ongoing",
      "role": "Lead Developer",
      "liveUrl": null,
      "sourceUrl": "https://github.com/ShriOg/nowhang",
      "pageUrl": "/pages/nowhang/",
      "status": "experimental",
      "featured": true,
      "order": 3,
      "sections": [
        {
          "id": "problem",
          "title": "Problem Statement",
          "content": [
            "Coordinating spontaneous hangouts is friction-heavy. Group chats turn into endless \"when works for you?\" loops. Calendar apps are designed for formal meetings, not casual social time.",
            "The core question: How do you surface real-time social availability without creating pressure or FOMO?"
          ]
        },
        {
          "id": "constraints",
          "title": "Constraints",
          "type": "list",
          "content": [
            "Must feel lighter than scheduling apps",
            "Real-time status updates without battery drain",
            "Privacy-respecting location features",
            "Works with existing friend groups (not social discovery)",
            "No notification spam"
          ]
        },
        {
          "id": "architecture",
          "title": "Architecture",
          "type": "diagram",
          "diagramTitle": "System Overview",
          "diagram": "Mobile App --> Firebase Auth --> Cloud Functions\n     |              |                    |\n     |         Real-time DB        Push Notifications\n     |              |                    |\n     +-----> Status Updates <-----+------+\n     |              |\n  Location    Friend Activity\n  (opt-in)      Feed",
          "content": [
            "Real-time sync through Firebase listeners. Location data is anonymized to approximate areas (neighborhood-level) rather than precise coordinates."
          ]
        },
        {
          "id": "status",
          "title": "Current Status",
          "type": "list",
          "content": [
            "Core status broadcasting working",
            "Friend groups with privacy levels",
            "Push notifications for matches",
            "Beta testing with 20 users",
            "Iterating on UX based on feedback"
          ]
        },
        {
          "id": "improvements",
          "title": "What I Would Improve",
          "type": "list",
          "content": [
            "Add ML-based activity suggestions",
            "Calendar integration for auto-availability",
            "Group status (\"3 of your friends are free\")",
            "Web companion app"
          ]
        }
      ]
    },
    {
      "id": "grades-dashboard",
      "title": "Grade Dashboard",
      "shortDescription": "A personal academic tracker with GPA calculations, grade visualization, and course history management.",
      "longDescription": "A personal academic tracking system that makes sense of transcript data. Real-time GPA calculations, grade trend visualization, and course organization.",
      "previewType": "grid",
      "techStack": ["HTML", "CSS", "JavaScript", "LocalStorage"],
      "techStackShort": "Web / Data Viz",
      "timeline": "2 weeks",
      "role": "Solo Developer",
      "liveUrl": "/pages/grades/",
      "sourceUrl": null,
      "pageUrl": "/pages/grades/",
      "status": "active",
      "featured": false,
      "order": 4,
      "sections": [
        {
          "id": "problem",
          "title": "Problem Statement",
          "content": [
            "University portals are clunky. I wanted a clean, personal dashboard to track my academic progress with proper GPA calculations and visualizations."
          ]
        },
        {
          "id": "outcome",
          "title": "Final Outcome",
          "type": "list",
          "content": [
            "Clean grade entry and management",
            "Accurate GPA calculations",
            "Visual grade distribution charts",
            "LocalStorage persistence"
          ]
        }
      ]
    },
    {
      "id": "gesture-mouse",
      "title": "Gesture-Controlled Mouse",
      "shortDescription": "Control your computer's cursor using hand gestures detected through a webcam. Built with MediaPipe and PyAutoGUI.",
      "longDescription": "A computer vision system that maps hand gestures to mouse controls. Point to move, pinch to click, scroll with two fingers—all through your webcam.",
      "previewType": "grid",
      "techStack": ["Python 3.11", "OpenCV", "MediaPipe", "PyAutoGUI", "NumPy"],
      "techStackShort": "Python / CV",
      "timeline": "4 weeks",
      "role": "Solo Developer",
      "liveUrl": null,
      "sourceUrl": "https://github.com/ShriOg/gesture-mouse",
      "pageUrl": "/pages/gesture-mouse/",
      "status": "active",
      "featured": false,
      "order": 5,
      "sections": [
        {
          "id": "problem",
          "title": "Problem Statement",
          "content": [
            "Physical mouse limitations in certain scenarios (presentations, accessibility needs, just wanting to wave at your computer). Gesture control could provide a natural, touchless alternative.",
            "The challenge: How do you make hand tracking stable enough for precise cursor control without expensive hardware?"
          ]
        },
        {
          "id": "approach",
          "title": "Approach",
          "content": [
            "Built a pipeline that processes webcam frames through MediaPipe's hand tracking, interprets specific finger configurations as gestures, and translates those into system-level mouse events via PyAutoGUI."
          ],
          "type": "list",
          "listContent": [
            "Real-time hand landmark detection at 30+ FPS",
            "Gesture classification based on finger positions and distances",
            "Exponential smoothing to eliminate jitter",
            "Debouncing logic to prevent false-positive clicks",
            "Configurable sensitivity zones and dead zones"
          ]
        },
        {
          "id": "architecture",
          "title": "Technical Implementation",
          "type": "diagram",
          "diagramTitle": "System Pipeline",
          "diagram": "Webcam Feed --> OpenCV Frame --> MediaPipe Hands --> Landmark Extraction\n                                                            |\n                                            +---------------+---------------+\n                                            |               |               |\n                                      Gesture          Position         State\n                                      Classifier       Smoother        Machine\n                                            |               |               |\n                                            +---------------+---------------+\n                                                            |\n                                                      PyAutoGUI\n                                                      (System Mouse)",
          "content": [
            "The state machine tracks gesture transitions to distinguish intentional actions from noise. Position smoothing uses a weighted moving average with configurable alpha values for responsiveness vs. stability tradeoff."
          ]
        },
        {
          "id": "gestures",
          "title": "Gesture Mapping Logic",
          "type": "table",
          "headers": ["Gesture", "Detection Method", "Action"],
          "rows": [
            ["Index finger extended", "Single fingertip above knuckle", "Cursor movement"],
            ["Index + Middle pinch", "Fingertip distance < threshold", "Left click"],
            ["Index + Thumb pinch", "Thumb-index distance collapse", "Right click"],
            ["Closed fist", "All fingertips below knuckles", "Drag start/stop"],
            ["Two fingers vertical motion", "Index + Middle Y-delta tracking", "Scroll up/down"]
          ]
        },
        {
          "id": "failures",
          "title": "What Failed",
          "content": [
            "<strong>Initial direct mapping:</strong> Mapping hand position directly to screen coordinates made small movements amplify dramatically. Switched to relative movement with adjustable sensitivity multiplier.",
            "<strong>Complex gesture vocabulary:</strong> Started with 8 gestures. Too many false positives between similar configurations. Reduced to 5 distinct, high-confidence gestures.",
            "<strong>Single-frame click detection:</strong> Caused double-clicks and phantom inputs. Required temporal consistency (gesture held for N frames) to confirm intent."
          ]
        },
        {
          "id": "outcome",
          "title": "Outcome & Learnings",
          "type": "list",
          "content": [
            "Stable cursor control with <50ms perceived latency",
            "95%+ gesture recognition accuracy under good lighting",
            "Works reliably at arm's length from standard webcams",
            "Configurable via CLI flags for sensitivity, smoothing, and detection thresholds",
            "Clean separation between detection, interpretation, and action layers"
          ],
          "additionalContent": [
            "Key insight: Human-computer interaction via gestures requires extensive filtering and state management. Raw sensor data is never directly usable—the intelligence is in the interpretation layer."
          ]
        },
        {
          "id": "improvements",
          "title": "What I Would Improve",
          "type": "list",
          "content": [
            "Add ML-based gesture classification for custom gesture training",
            "Implement two-hand support for modifier keys (Ctrl, Shift, Alt)",
            "System tray integration with hotkey toggle",
            "Calibration wizard for personalized sensitivity",
            "WebSocket API for integration with other applications"
          ]
        }
      ]
    },
    {
      "id": "imported-chat-viewer",
      "title": "Imported Chat Viewer",
      "shortDescription": "A platform-accurate WhatsApp & Instagram chat viewer for style-based AI training.",
      "longDescription": "A generic chat import system that renders WhatsApp and Instagram style conversations with sender role assignment and privacy-safe local processing.",
      "previewType": "grid",
      "techStack": ["HTML", "CSS", "Vanilla JavaScript", "IndexedDB"],
      "techStackShort": "JS / IndexedDB",
      "timeline": "2 weeks",
      "role": "Solo Developer",
      "liveUrl": "/private/personal.html",
      "sourceUrl": null,
      "pageUrl": null,
      "status": "active",
      "featured": true,
      "order": 6,
      "sections": [
        {
          "id": "problem",
          "title": "Problem Statement",
          "content": [
            "Training an AI persona requires reference to real conversation patterns, but existing tools either expose private data or lack platform-accurate rendering.",
            "The challenge: How do you build a generic chat parser that works with any WhatsApp or Instagram export while keeping all data local?"
          ]
        },
        {
          "id": "features",
          "title": "Key Features",
          "type": "list",
          "content": [
            "WhatsApp-style & Instagram-style rendering",
            "Generic chat parser (date, time, sender, message)",
            "Sender role assignment during import",
            "No hardcoded names - works with any chat",
            "Privacy-safe, local-only processing",
            "Mobile-first responsive UI"
          ]
        },
        {
          "id": "outcome",
          "title": "Final Outcome",
          "type": "list",
          "content": [
            "Standardized parser supporting DD/MM/YY format",
            "Two-step import with participant role selection",
            "Platform-specific bubble styling",
            "File upload and paste support",
            "Persistent storage via IndexedDB"
          ]
        }
      ]
    },
    {
      "id": "her-mode-ai",
      "title": "Her Mode AI",
      "shortDescription": "An emotion-first Hinglish conversational AI designed with human-like pacing and warmth.",
      "longDescription": "A style-trained emotional AI that speaks in Hinglish, responds with human-like pacing, and prioritizes emotional connection over task completion.",
      "previewType": "waves",
      "techStack": ["HTML", "CSS", "Vanilla JavaScript", "IndexedDB"],
      "techStackShort": "JS / AI Design",
      "timeline": "Ongoing",
      "role": "Solo Developer",
      "liveUrl": "/private/personal.html",
      "sourceUrl": null,
      "pageUrl": null,
      "status": "active",
      "featured": true,
      "order": 7,
      "sections": [
        {
          "id": "problem",
          "title": "Problem Statement",
          "content": [
            "Most AI assistants sound robotic and professional. I wanted to build something that feels warm, understands emotional context, and speaks naturally in Hinglish.",
            "The challenge: How do you train an AI to respond emotionally first without copying private conversations?"
          ]
        },
        {
          "id": "features",
          "title": "Key Features",
          "type": "list",
          "content": [
            "Emotion-first response engine",
            "Hinglish conversational flow",
            "Human pacing & micro-reactions",
            "Style-trained using real chat patterns",
            "Safe, non-manipulative behavior design",
            "Mobile-optimized chat UI"
          ]
        },
        {
          "id": "approach",
          "title": "Design Philosophy",
          "content": [
            "Instead of copying conversations, the system learns response patterns: how to acknowledge emotion, when to use emojis, how to pace multi-part responses.",
            "The goal is warmth without deception—an AI that feels present but never pretends to be human."
          ]
        },
        {
          "id": "outcome",
          "title": "Final Outcome",
          "type": "list",
          "content": [
            "Natural Hinglish responses with emotional awareness",
            "Typing indicators with human-like delays",
            "Context-aware conversation memory",
            "Style training UI for pattern refinement",
            "Privacy-first local processing"
          ]
        }
      ]
    }
  ],

  "footer": {
    "links": [
      { "label": "GitHub", "href": "https://github.com/ShriOg", "external": true },
      { "label": "LinkedIn", "href": "https://linkedin.com/in/anandshukla", "external": true },
      { "label": "Email", "href": "mailto:hello@anand.dev", "external": false }
    ],
    "copyright": "© 2025 Anand Shukla. Built with intention."
  },

  "icons": {
    "play": "<svg viewBox=\"0 0 24 24\"><polygon points=\"5 3 19 12 5 21 5 3\"></polygon></svg>",
    "github": "<svg viewBox=\"0 0 24 24\"><path d=\"M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22\"></path></svg>",
    "external": "<svg viewBox=\"0 0 24 24\"><path d=\"M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6\"></path><polyline points=\"15 3 21 3 21 9\"></polyline><line x1=\"10\" y1=\"14\" x2=\"21\" y2=\"3\"></line></svg>",
    "docs": "<svg viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\"><path d=\"M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z\"></path><polyline points=\"14 2 14 8 20 8\"></polyline><line x1=\"16\" y1=\"13\" x2=\"8\" y2=\"13\"></line><line x1=\"16\" y1=\"17\" x2=\"8\" y2=\"17\"></line><polyline points=\"10 9 9 9 8 9\"></polyline></svg>",
    "arrow": "→",
    "close": "×"
  }
}
