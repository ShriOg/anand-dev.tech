<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Projects | Anand Dev OS</title>
  <meta name="description" content="Case studies and engineering stories from projects I have built.">
  <meta name="theme-color" content="#0a0a0b">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="styles.css">
</head>
<body class="page">
  <nav class="nav">
    <div class="nav__inner">
      <a href="index.html" class="nav__logo">anand<span>.</span>dev</a>
      <button class="nav__toggle" aria-label="Toggle menu">
        <span></span>
        <span></span>
        <span></span>
      </button>
      <div class="nav__links">
        <a href="index.html" class="nav__link">Home</a>
        <a href="projects.html" class="nav__link nav__link--active">Projects</a>
        <a href="lab.html" class="nav__link">Lab</a>
        <a href="dev-os.html" class="nav__link">Dev OS</a>
        <a href="hire.html" class="nav__link">Hire Me</a>
      </div>
    </div>
  </nav>

  <main style="padding-top: var(--space-24);">
    <div class="container">
      <header class="section-header reveal">
        <span class="section-header__eyebrow">Projects</span>
        <h1 class="section-header__title">Engineering Stories</h1>
        <p class="section-header__description">
          Not just what I built, but how I thought through problems, what failed, 
          and what I learned along the way. Click any card to explore the full case study.
        </p>
      </header>

      <!-- Focus Cards Grid -->
      <div class="focus-cards-grid">

        <!-- Card 1: AI Desktop Assistant -->
        <article class="focus-card reveal" id="ai-desktop-assistant" data-focus-id="ai-desktop-assistant">
          <div class="focus-card__preview">
            <div class="focus-card__preview-visual">
              <canvas data-preview="grid"></canvas>
            </div>
          </div>
          <div class="focus-card__body">
            <span class="focus-card__tag">Python / NLP</span>
            <h2 class="focus-card__title">AI Desktop Assistant</h2>
            <p class="focus-card__description">
              A voice-controlled assistant that understands context and executes system commands across Windows, macOS, and Linux.
            </p>
            <div class="focus-card__meta">
              <span class="focus-card__meta-item">Timeline: <span>3 months</span></span>
              <span class="focus-card__meta-item">Role: <span>Solo Developer</span></span>
            </div>
            <div class="focus-card__actions">
              <a href="#" class="action-btn action-btn--primary action-btn--disabled" aria-disabled="true" tabindex="-1">
                <span class="action-btn__icon"><svg viewBox="0 0 24 24"><polygon points="5 3 19 12 5 21 5 3"></polygon></svg></span>
                View Live
              </a>
              <a href="https://github.com/ShriOg/anand-dev.tech/tree/main/projects/ai-assistant" target="_blank" rel="noopener noreferrer" class="action-btn action-btn--external">
                <span class="action-btn__icon"><svg viewBox="0 0 24 24"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></span>
                View Source
              </a>
            </div>
          </div>
          <div class="focus-card__arrow">→</div>

          <!-- Hidden content template for overlay -->
          <template class="focus-card__content-template">
            <header class="focus-overlay__header">
              <div class="focus-overlay__meta">
                <div class="focus-overlay__meta-item">
                  <span class="focus-overlay__meta-label">Timeline</span>
                  <span class="focus-overlay__meta-value">3 months</span>
                </div>
                <div class="focus-overlay__meta-item">
                  <span class="focus-overlay__meta-label">Role</span>
                  <span class="focus-overlay__meta-value">Solo Developer</span>
                </div>
                <div class="focus-overlay__meta-item">
                  <span class="focus-overlay__meta-label">Stack</span>
                  <span class="focus-overlay__meta-value">Python, NLP</span>
                </div>
              </div>
              <h2 class="focus-overlay__title">AI Desktop Assistant</h2>
              <p class="focus-overlay__summary">
                Building a voice-controlled assistant that actually understands context 
                and executes system commands across Windows, macOS, and Linux.
              </p>
              <div class="focus-overlay__actions">
                <a href="#" class="action-btn action-btn--primary action-btn--disabled" aria-disabled="true" tabindex="-1">
                  <span class="action-btn__icon"><svg viewBox="0 0 24 24"><polygon points="5 3 19 12 5 21 5 3"></polygon></svg></span>
                  View Live Demo
                </a>
                <a href="https://github.com/ShriOg/anand-dev.tech/tree/main/projects/ai-assistant" target="_blank" rel="noopener noreferrer" class="action-btn action-btn--external">
                  <span class="action-btn__icon"><svg viewBox="0 0 24 24"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></span>
                  View Source on GitHub
                </a>
              </div>
            </header>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">Problem Statement</h3>
              <div class="focus-overlay__section-content">
                <p>
                  Existing voice assistants are either cloud-dependent, platform-locked, 
                  or require complex setup. I wanted something that could run locally, 
                  understand natural variations of commands, and actually control my computer.
                </p>
                <p>
                  The core challenge: How do you map fuzzy human language to precise system commands 
                  while maintaining context across a conversation?
                </p>
              </div>
            </section>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">Constraints</h3>
              <div class="focus-overlay__section-content">
                <ul>
                  <li>Must run entirely offline after initial setup</li>
                  <li>Cross-platform support (Windows, macOS, Linux)</li>
                  <li>Response time under 500ms for common commands</li>
                  <li>Extensible action system without code changes</li>
                  <li>No external API dependencies for core functionality</li>
                </ul>
              </div>
            </section>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">Architecture</h3>
              <div class="focus-overlay__section-content">
                <div class="architecture-diagram">
                  <div class="architecture-diagram__title">System Overview</div>
                  <div class="architecture-diagram__content">
Voice Input --> Speech-to-Text --> Intent Parser --> Router
                                                       |
                                        +--------------+--------------+
                                        |              |              |
                                    Context       Action Schema    Adapter
                                    Manager       Validator       (OS-specific)
                                        |              |              |
                                        +--------------+--------------+
                                                       |
                                                   Executor --> System</div>
                </div>
                <p>
                  The key insight was separating intent parsing from action execution. 
                  The router maintains conversation context while the adapters handle 
                  platform-specific implementations.
                </p>
              </div>
            </section>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">Tradeoffs</h3>
              <div class="focus-overlay__section-content">
                <table class="tradeoff-table">
                  <thead>
                    <tr>
                      <th>Decision</th>
                      <th>Benefit</th>
                      <th>Cost</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>Local-first processing</td>
                      <td>Privacy, offline capability</td>
                      <td>Limited NLP accuracy vs cloud</td>
                    </tr>
                    <tr>
                      <td>Schema-based actions</td>
                      <td>Type safety, validation</td>
                      <td>More boilerplate per action</td>
                    </tr>
                    <tr>
                      <td>Adapter pattern for OS</td>
                      <td>Clean abstraction</td>
                      <td>Maintenance across 3 platforms</td>
                    </tr>
                  </tbody>
                </table>
              </div>
            </section>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">What Failed</h3>
              <div class="focus-overlay__section-content">
                <p>
                  <strong>First attempt at intent parsing:</strong> Started with regex patterns. 
                  Worked for exact matches but completely broke with any variation. 
                  "Open Chrome" worked, "launch chrome browser" didn't.
                </p>
                <p>
                  <strong>Context management v1:</strong> Stored everything in a flat dictionary. 
                  Memory grew unbounded and older context polluted new interactions.
                </p>
                <p>
                  <strong>Windows adapter initial approach:</strong> Used subprocess for everything. 
                  Some commands need COM objects, others need PowerShell. Unified interface was naive.
                </p>
              </div>
            </section>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">Final Outcome</h3>
              <div class="focus-overlay__section-content">
                <ul>
                  <li>Working assistant with 50+ supported commands</li>
                  <li>Average response time: 180ms</li>
                  <li>Intent recognition accuracy: ~85% on varied phrasing</li>
                  <li>Clean plugin system for adding new actions</li>
                  <li>Documented API for external integrations</li>
                </ul>
              </div>
            </section>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">What I Would Improve</h3>
              <div class="focus-overlay__section-content">
                <ul>
                  <li>Add lightweight local LLM option for better intent parsing</li>
                  <li>Implement action chaining (e.g., "open browser and go to github")</li>
                  <li>Better error recovery with suggested alternatives</li>
                  <li>Visual feedback system for non-terminal usage</li>
                </ul>
              </div>
            </section>

            <div class="focus-overlay__tech-stack">
              <span class="tech-stack__item">Python 3.11</span>
              <span class="tech-stack__item">Pydantic</span>
              <span class="tech-stack__item">SpeechRecognition</span>
              <span class="tech-stack__item">pywin32</span>
              <span class="tech-stack__item">subprocess</span>
            </div>
          </template>
        </article>

        <!-- Card 2: Gesture-Controlled Particle System -->
        <article class="focus-card reveal" id="particle-system-gestures" data-focus-id="particle-system-gestures">
          <div class="focus-card__preview">
            <div class="focus-card__preview-visual">
              <canvas data-preview="particles"></canvas>
            </div>
          </div>
          <div class="focus-card__body">
            <span class="focus-card__tag">JS / Canvas / MediaPipe</span>
            <h2 class="focus-card__title">Gesture-Controlled Particles</h2>
            <p class="focus-card__description">
              An interactive visual system where hand gestures control particle behavior in real-time using computer vision.
            </p>
            <div class="focus-card__meta">
              <span class="focus-card__meta-item">Timeline: <span>6 weeks</span></span>
              <span class="focus-card__meta-item">Role: <span>Solo Developer</span></span>
            </div>
            <div class="focus-card__actions">
              <a href="assets/particle-system.html" target="_blank" rel="noopener" class="action-btn action-btn--primary">
                <span class="action-btn__icon"><svg viewBox="0 0 24 24"><polygon points="5 3 19 12 5 21 5 3"></polygon></svg></span>
                View Live
              </a>
              <a href="https://github.com/ShriOg/particle-system-guestures" target="_blank" rel="noopener noreferrer" class="action-btn action-btn--external">
                <span class="action-btn__icon"><svg viewBox="0 0 24 24"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></span>
                View Source
              </a>
            </div>
          </div>
          <div class="focus-card__arrow">→</div>

          <!-- Hidden content template for overlay -->
          <template class="focus-card__content-template">
            <header class="focus-overlay__header">
              <div class="focus-overlay__meta">
                <div class="focus-overlay__meta-item">
                  <span class="focus-overlay__meta-label">Timeline</span>
                  <span class="focus-overlay__meta-value">6 weeks</span>
                </div>
                <div class="focus-overlay__meta-item">
                  <span class="focus-overlay__meta-label">Role</span>
                  <span class="focus-overlay__meta-value">Solo Developer</span>
                </div>
                <div class="focus-overlay__meta-item">
                  <span class="focus-overlay__meta-label">Stack</span>
                  <span class="focus-overlay__meta-value">JS, Canvas, MediaPipe</span>
                </div>
              </div>
              <h2 class="focus-overlay__title">Gesture-Controlled Particle System</h2>
              <p class="focus-overlay__summary">
                An interactive visual system where hand gestures control particle behavior 
                in real-time. Exploring the intersection of computer vision and creative coding.
              </p>
              <div class="focus-overlay__actions">
                <a href="assets/particle-system.html" target="_blank" rel="noopener" class="action-btn action-btn--primary action-btn--large">
                  <span class="action-btn__icon"><svg viewBox="0 0 24 24"><polygon points="5 3 19 12 5 21 5 3"></polygon></svg></span>
                  View Live Demo
                </a>
                <a href="https://github.com/ShriOg/particle-system-guestures" target="_blank" rel="noopener noreferrer" class="action-btn action-btn--external action-btn--large">
                  <span class="action-btn__icon"><svg viewBox="0 0 24 24"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></span>
                  View Source Code
                </a>
              </div>
            </header>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">Problem Statement</h3>
              <div class="focus-overlay__section-content">
                <p>
                  Most interactive demos use mouse/touch input. I wanted to explore how 
                  gesture recognition could create more intuitive and expressive interactions.
                </p>
                <p>
                  The challenge: MediaPipe runs at ~30fps for hand tracking, but smooth 
                  particle animation needs 60fps. How do you bridge this gap without 
                  visible stuttering?
                </p>
              </div>
            </section>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">Constraints</h3>
              <div class="focus-overlay__section-content">
                <ul>
                  <li>Must run in browser without plugins</li>
                  <li>60fps rendering minimum</li>
                  <li>Support 500+ simultaneous particles</li>
                  <li>Gesture detection latency under 100ms perceived</li>
                  <li>Graceful degradation on lower-end devices</li>
                </ul>
              </div>
            </section>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">Architecture</h3>
              <div class="focus-overlay__section-content">
                <div class="architecture-diagram">
                  <div class="architecture-diagram__title">Pipeline</div>
                  <div class="architecture-diagram__content">
Camera --> MediaPipe --> Gesture      --> Particle System
           (30fps)     Interpreter        (60fps)
                           |                  |
                      Hand State         Physics Engine
                      Interpolator       Spatial Hash
                           |                  |
                       Smoothed          Optimized
                       Position          Collision</div>
                </div>
                <p>
                  Key innovation: Position interpolation between MediaPipe frames. 
                  The gesture interpreter predicts hand position for intermediate frames, 
                  creating smooth 60fps interaction from 30fps tracking.
                </p>
              </div>
            </section>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">Tradeoffs</h3>
              <div class="focus-overlay__section-content">
                <table class="tradeoff-table">
                  <thead>
                    <tr>
                      <th>Decision</th>
                      <th>Benefit</th>
                      <th>Cost</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>Canvas 2D over WebGL</td>
                      <td>Simpler code, wider support</td>
                      <td>Particle limit ~800 vs thousands</td>
                    </tr>
                    <tr>
                      <td>Position interpolation</td>
                      <td>Smooth 60fps feel</td>
                      <td>Slight prediction errors visible</td>
                    </tr>
                    <tr>
                      <td>Spatial hashing</td>
                      <td>O(n) collision detection</td>
                      <td>Memory overhead for grid</td>
                    </tr>
                  </tbody>
                </table>
              </div>
            </section>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">What Failed</h3>
              <div class="focus-overlay__section-content">
                <p>
                  <strong>Direct MediaPipe coordinates:</strong> Hand jitter made particles 
                  shake uncontrollably. Needed velocity-based smoothing with configurable damping.
                </p>
                <p>
                  <strong>Naive collision detection:</strong> O(n²) killed performance at 
                  300+ particles. Spatial hashing brought it back to 60fps.
                </p>
                <p>
                  <strong>Gesture recognition complexity:</strong> Started with 10 gestures. 
                  Too many false positives. Reduced to 4 reliable gestures with clear visual states.
                </p>
              </div>
            </section>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">Final Outcome</h3>
              <div class="focus-overlay__section-content">
                <ul>
                  <li>Smooth 60fps with 500 particles</li>
                  <li>4 gesture types: attract, repel, swirl, explode</li>
                  <li>Perceived latency under 50ms</li>
                  <li>Works on mobile with touch fallback</li>
                  <li>Configurable physics parameters via UI</li>
                </ul>
              </div>
            </section>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">What I Would Improve</h3>
              <div class="focus-overlay__section-content">
                <ul>
                  <li>WebGL renderer for 10x particle count</li>
                  <li>WebWorker for physics calculations</li>
                  <li>Custom gesture training for user-defined interactions</li>
                  <li>Audio-reactive particle behaviors</li>
                </ul>
              </div>
            </section>

            <div class="focus-overlay__tech-stack">
              <span class="tech-stack__item">Vanilla JavaScript</span>
              <span class="tech-stack__item">Canvas 2D</span>
              <span class="tech-stack__item">MediaPipe Hands</span>
              <span class="tech-stack__item">requestAnimationFrame</span>
            </div>
          </template>
        </article>

        <!-- Card 3: NowHang -->
        <article class="focus-card reveal" id="nowhang" data-focus-id="nowhang">
          <div class="focus-card__preview">
            <div class="focus-card__preview-visual">
              <canvas data-preview="waves"></canvas>
            </div>
          </div>
          <div class="focus-card__body">
            <span class="focus-card__tag">Full Stack / Real-time</span>
            <h2 class="focus-card__title">NowHang</h2>
            <p class="focus-card__description">
              A social coordination app that eliminates planning friction by matching real-time availability and interests.
            </p>
            <div class="focus-card__meta">
              <span class="focus-card__meta-item">Timeline: <span>Ongoing</span></span>
              <span class="focus-card__meta-item">Role: <span>Lead Developer</span></span>
            </div>
            <div class="focus-card__actions">
              <span class="action-btn action-btn--disabled" aria-disabled="true">
                <span class="action-btn__icon"><svg viewBox="0 0 24 24"><polygon points="5 3 19 12 5 21 5 3"></polygon></svg></span>
                Coming Soon
              </span>
              <a href="https://github.com/ShriOg/nowhang" target="_blank" rel="noopener noreferrer" class="action-btn action-btn--external">
                <span class="action-btn__icon"><svg viewBox="0 0 24 24"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></span>
                View Source
              </a>
            </div>
          </div>
          <div class="focus-card__arrow">→</div>

          <!-- Hidden content template for overlay -->
          <template class="focus-card__content-template">
            <header class="focus-overlay__header">
              <div class="focus-overlay__meta">
                <div class="focus-overlay__meta-item">
                  <span class="focus-overlay__meta-label">Timeline</span>
                  <span class="focus-overlay__meta-value">Ongoing</span>
                </div>
                <div class="focus-overlay__meta-item">
                  <span class="focus-overlay__meta-label">Role</span>
                  <span class="focus-overlay__meta-value">Lead Developer</span>
                </div>
                <div class="focus-overlay__meta-item">
                  <span class="focus-overlay__meta-label">Stack</span>
                  <span class="focus-overlay__meta-value">Full Stack</span>
                </div>
              </div>
              <h2 class="focus-overlay__title">NowHang</h2>
              <p class="focus-overlay__summary">
                A social coordination app that eliminates the friction of planning hangouts 
                by matching real-time availability and interests.
              </p>
              <div class="focus-overlay__actions">
                <span class="action-btn action-btn--disabled action-btn--large" aria-disabled="true">
                  <span class="action-btn__icon"><svg viewBox="0 0 24 24"><polygon points="5 3 19 12 5 21 5 3"></polygon></svg></span>
                  Demo Coming Soon
                </span>
                <a href="https://github.com/ShriOg/nowhang" target="_blank" rel="noopener noreferrer" class="action-btn action-btn--external action-btn--large">
                  <span class="action-btn__icon"><svg viewBox="0 0 24 24"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></span>
                  View Source Code
                </a>
              </div>
            </header>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">Problem Statement</h3>
              <div class="focus-overlay__section-content">
                <p>
                  Group chats devolve into endless "when are you free?" threads. 
                  Calendar apps are for scheduled events, not spontaneous hangouts. 
                  There's no good tool for "I'm free right now, who else is?"
                </p>
                <p>
                  The challenge: Real-time availability matching with minimal user friction. 
                  People won't open an app just to say they're free unless it's dead simple.
                </p>
              </div>
            </section>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">Constraints</h3>
              <div class="focus-overlay__section-content">
                <ul>
                  <li>One-tap availability broadcast</li>
                  <li>Real-time matching (sub-second updates)</li>
                  <li>Privacy-first: location only when explicitly shared</li>
                  <li>Works with small friend groups (5-20 people)</li>
                  <li>Minimal notification fatigue</li>
                </ul>
              </div>
            </section>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">Architecture</h3>
              <div class="focus-overlay__section-content">
                <div class="architecture-diagram">
                  <div class="architecture-diagram__title">Real-time Matching</div>
                  <div class="architecture-diagram__content">
Client (PWA) --> WebSocket --> Availability Pool
                                     |
                              Matching Engine
                              (Interest + Time + Location)
                                     |
                              Push Notification
                              Service</div>
                </div>
                <p>
                  WebSocket connection stays alive when app is foregrounded. 
                  Background sync handles availability expiration. 
                  Matching runs on write, not polling.
                </p>
              </div>
            </section>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">What Failed</h3>
              <div class="focus-overlay__section-content">
                <p>
                  <strong>Activity categories v1:</strong> Too many options paralyzed users. 
                  Reduced from 20 categories to 5 broad types with optional details.
                </p>
                <p>
                  <strong>Always-on availability:</strong> Users forgot to turn it off, 
                  leading to unwanted notifications. Added auto-expiry with adjustable duration.
                </p>
                <p>
                  <strong>Group size assumptions:</strong> Designed for large networks initially. 
                  Pivoted to close friends focus after user feedback showed that's what people wanted.
                </p>
              </div>
            </section>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">Current Status</h3>
              <div class="focus-overlay__section-content">
                <ul>
                  <li>MVP live with 50 beta users</li>
                  <li>Average session: 12 seconds to broadcast availability</li>
                  <li>Match rate: 34% of broadcasts result in a hangout</li>
                  <li>Iterating on notification timing and matching algorithm</li>
                </ul>
              </div>
            </section>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">What I Would Improve</h3>
              <div class="focus-overlay__section-content">
                <ul>
                  <li>Smart suggestions based on past hangout patterns</li>
                  <li>Calendar integration for automatic availability detection</li>
                  <li>Group availability visualization</li>
                  <li>Venue suggestions based on group location midpoint</li>
                </ul>
              </div>
            </section>

            <div class="focus-overlay__tech-stack">
              <span class="tech-stack__item">React</span>
              <span class="tech-stack__item">Node.js</span>
              <span class="tech-stack__item">WebSocket</span>
              <span class="tech-stack__item">PostgreSQL</span>
              <span class="tech-stack__item">PWA</span>
            </div>
          </template>
        </article>

        <!-- Card 4: Hand Gesture Mouse Control -->
        <article class="focus-card reveal" id="hand-gesture-mouse-control" data-focus-id="hand-gesture-mouse-control">
          <div class="focus-card__preview">
            <div class="focus-card__preview-visual">
              <canvas data-preview="grid"></canvas>
            </div>
          </div>
          <div class="focus-card__body">
            <span class="focus-card__tag">Python / Computer Vision</span>
            <h2 class="focus-card__title">Hand Gesture Mouse Control</h2>
            <p class="focus-card__description">
              A real-time computer vision system that controls the mouse cursor, clicks, scrolling, and drag operations using hand gestures via webcam.
            </p>
            <div class="focus-card__meta">
              <span class="focus-card__meta-item">Timeline: <span>4 weeks</span></span>
              <span class="focus-card__meta-item">Role: <span>Solo Developer</span></span>
            </div>
            <div class="focus-card__actions">
              <a href="#" class="action-btn action-btn--primary action-btn--disabled" aria-disabled="true" tabindex="-1">
                <span class="action-btn__icon"><svg viewBox="0 0 24 24"><polygon points="5 3 19 12 5 21 5 3"></polygon></svg></span>
                View Live
              </a>
              <a href="https://github.com/ShriOg/hand-gesture-control" target="_blank" rel="noopener noreferrer" class="action-btn action-btn--external">
                <span class="action-btn__icon"><svg viewBox="0 0 24 24"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></span>
                View Source
              </a>
            </div>
          </div>
          <div class="focus-card__arrow">→</div>

          <!-- Hidden content template for overlay -->
          <template class="focus-card__content-template">
            <header class="focus-overlay__header">
              <div class="focus-overlay__meta">
                <div class="focus-overlay__meta-item">
                  <span class="focus-overlay__meta-label">Timeline</span>
                  <span class="focus-overlay__meta-value">4 weeks</span>
                </div>
                <div class="focus-overlay__meta-item">
                  <span class="focus-overlay__meta-label">Role</span>
                  <span class="focus-overlay__meta-value">Solo Developer</span>
                </div>
                <div class="focus-overlay__meta-item">
                  <span class="focus-overlay__meta-label">Stack</span>
                  <span class="focus-overlay__meta-value">Python, OpenCV, MediaPipe</span>
                </div>
              </div>
              <h2 class="focus-overlay__title">Hand Gesture Mouse Control</h2>
              <p class="focus-overlay__summary">
                A real-time computer vision system that allows controlling the mouse cursor, 
                clicks, scrolling, and drag operations using hand gestures captured via webcam—direct 
                system-level control without any physical input device.
              </p>
              <div class="focus-overlay__actions">
                <a href="#" class="action-btn action-btn--primary action-btn--disabled action-btn--large" aria-disabled="true" tabindex="-1">
                  <span class="action-btn__icon"><svg viewBox="0 0 24 24"><polygon points="5 3 19 12 5 21 5 3"></polygon></svg></span>
                  View Live Demo
                </a>
                <a href="https://github.com/ShriOg/hand-gesture-control" target="_blank" rel="noopener noreferrer" class="action-btn action-btn--external action-btn--large">
                  <span class="action-btn__icon"><svg viewBox="0 0 24 24"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></span>
                  View Source on GitHub
                </a>
              </div>
            </header>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">Problem Statement</h3>
              <div class="focus-overlay__section-content">
                <p>
                  Traditional input devices require physical contact and precise motor control. 
                  For accessibility scenarios, presentations, or simply hands-free computing, 
                  there's a need for touchless computer interaction that feels natural and responsive.
                </p>
                <p>
                  The core challenge: How do you translate imprecise, jittery hand movements from 
                  a webcam feed into precise, stable cursor control while preventing accidental 
                  clicks and maintaining real-time performance?
                </p>
              </div>
            </section>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">Approach</h3>
              <div class="focus-overlay__section-content">
                <p>
                  Built a pipeline that processes webcam frames through MediaPipe's hand tracking, 
                  interprets specific finger configurations as gestures, and translates those into 
                  system-level mouse events via PyAutoGUI.
                </p>
                <ul>
                  <li>Real-time hand landmark detection at 30+ FPS</li>
                  <li>Gesture classification based on finger positions and distances</li>
                  <li>Exponential smoothing to eliminate jitter</li>
                  <li>Debouncing logic to prevent false-positive clicks</li>
                  <li>Configurable sensitivity zones and dead zones</li>
                </ul>
              </div>
            </section>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">Technical Implementation</h3>
              <div class="focus-overlay__section-content">
                <div class="architecture-diagram">
                  <div class="architecture-diagram__title">System Pipeline</div>
                  <div class="architecture-diagram__content">
Webcam Feed --> OpenCV Frame --> MediaPipe Hands --> Landmark Extraction
                                                            |
                                            +---------------+---------------+
                                            |               |               |
                                      Gesture          Position         State
                                      Classifier       Smoother        Machine
                                            |               |               |
                                            +---------------+---------------+
                                                            |
                                                      PyAutoGUI
                                                      (System Mouse)</div>
                </div>
                <p>
                  The state machine tracks gesture transitions to distinguish intentional actions 
                  from noise. Position smoothing uses a weighted moving average with configurable 
                  alpha values for responsiveness vs. stability tradeoff.
                </p>
              </div>
            </section>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">Gesture Mapping Logic</h3>
              <div class="focus-overlay__section-content">
                <table class="tradeoff-table">
                  <thead>
                    <tr>
                      <th>Gesture</th>
                      <th>Detection Method</th>
                      <th>Action</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>Index finger extended</td>
                      <td>Single fingertip above knuckle</td>
                      <td>Cursor movement</td>
                    </tr>
                    <tr>
                      <td>Index + Middle pinch</td>
                      <td>Fingertip distance &lt; threshold</td>
                      <td>Left click</td>
                    </tr>
                    <tr>
                      <td>Index + Thumb pinch</td>
                      <td>Thumb-index distance collapse</td>
                      <td>Right click</td>
                    </tr>
                    <tr>
                      <td>Closed fist</td>
                      <td>All fingertips below knuckles</td>
                      <td>Drag start/stop</td>
                    </tr>
                    <tr>
                      <td>Two fingers vertical motion</td>
                      <td>Index + Middle Y-delta tracking</td>
                      <td>Scroll up/down</td>
                    </tr>
                  </tbody>
                </table>
              </div>
            </section>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">Performance & Stability Considerations</h3>
              <div class="focus-overlay__section-content">
                <p>
                  <strong>Jitter elimination:</strong> Raw landmark coordinates fluctuate by 5-15 pixels 
                  between frames even with a stationary hand. Applied exponential moving average 
                  with α=0.3, reducing perceived jitter to &lt;2 pixels.
                </p>
                <p>
                  <strong>Click debouncing:</strong> Pinch gestures can oscillate near the threshold. 
                  Implemented a 150ms cooldown between click events and required gesture stability 
                  for 3 consecutive frames before triggering.
                </p>
                <p>
                  <strong>Frame rate optimization:</strong> MediaPipe processing is GPU-accelerated 
                  where available. Added frame skipping option for lower-end systems, maintaining 
                  gesture responsiveness at 15 FPS minimum.
                </p>
                <p>
                  <strong>Safety bounds:</strong> Cursor movement is constrained to screen boundaries 
                  with configurable edge buffers to prevent accidental clicks on system UI elements.
                </p>
              </div>
            </section>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">What Failed</h3>
              <div class="focus-overlay__section-content">
                <p>
                  <strong>Initial direct mapping:</strong> Mapping hand position directly to screen 
                  coordinates made small movements amplify dramatically. Switched to relative 
                  movement with adjustable sensitivity multiplier.
                </p>
                <p>
                  <strong>Complex gesture vocabulary:</strong> Started with 8 gestures. Too many 
                  false positives between similar configurations. Reduced to 5 distinct, 
                  high-confidence gestures.
                </p>
                <p>
                  <strong>Single-frame click detection:</strong> Caused double-clicks and phantom 
                  inputs. Required temporal consistency (gesture held for N frames) to confirm intent.
                </p>
              </div>
            </section>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">Outcome & Learnings</h3>
              <div class="focus-overlay__section-content">
                <ul>
                  <li>Stable cursor control with &lt;50ms perceived latency</li>
                  <li>95%+ gesture recognition accuracy under good lighting</li>
                  <li>Works reliably at arm's length from standard webcams</li>
                  <li>Configurable via CLI flags for sensitivity, smoothing, and detection thresholds</li>
                  <li>Clean separation between detection, interpretation, and action layers</li>
                </ul>
                <p>
                  Key insight: Human-computer interaction via gestures requires extensive 
                  filtering and state management. Raw sensor data is never directly usable—the 
                  intelligence is in the interpretation layer.
                </p>
              </div>
            </section>

            <section class="focus-overlay__section">
              <h3 class="focus-overlay__section-title">What I Would Improve</h3>
              <div class="focus-overlay__section-content">
                <ul>
                  <li>Add ML-based gesture classification for custom gesture training</li>
                  <li>Implement two-hand support for modifier keys (Ctrl, Shift, Alt)</li>
                  <li>System tray integration with hotkey toggle</li>
                  <li>Calibration wizard for personalized sensitivity</li>
                  <li>WebSocket API for integration with other applications</li>
                </ul>
              </div>
            </section>

            <div class="focus-overlay__tech-stack">
              <span class="tech-stack__item">Python 3.11</span>
              <span class="tech-stack__item">OpenCV</span>
              <span class="tech-stack__item">MediaPipe</span>
              <span class="tech-stack__item">PyAutoGUI</span>
              <span class="tech-stack__item">NumPy</span>
            </div>
          </template>
        </article>

      </div><!-- /.focus-cards-grid -->

    </div>
  </main>

  <footer class="footer">
    <div class="container">
      <div class="footer__inner">
        <div class="footer__links">
          <a href="https://github.com/ShriOg" class="footer__link" target="_blank" rel="noopener">GitHub</a>
          <a href="https://linkedin.com/in/anandshukla" class="footer__link" target="_blank" rel="noopener">LinkedIn</a>
          <a href="mailto:hello@anand.dev" class="footer__link">Email</a>
        </div>
        <p class="footer__copyright">© 2025 Anand Shukla. Built with intention.</p>
      </div>
    </div>
  </footer>

  <script src="script.js"></script>
</body>
</html>
